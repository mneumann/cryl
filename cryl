#!/usr/bin/env ruby

require 'optparse'
require 'ostruct'
require 'fileutils'
$LOAD_PATH.unshift File.expand_path(File.dirname(__FILE__))
require 'cryl'

options = OpenStruct.new 
options.root_dir = File.expand_path(File.join(Dir.pwd, "work"))
options.job_name = nil
options.urls = nil
options.depth = 2
options.cat_domain = nil

options.processes = 1

options.keep_rejected_urls = false
options.compile = false
options.daemonize = false

opts = OptionParser.new do |opts|
  opts.banner = "Usage: cryl [options]"

  opts.on("-r", "--root-dir DIR", "Directory to store the crawl into",
                                  "  (default: #{options.root_dir})") do |r|
    options.root_dir = File.expand_path(r)
  end

  opts.on("-j", "--job-name NAME", "Name of the Job (required)") do |j|
    options.job_name = j
  end

  opts.on("-u", "--urls FILE", "File containing the URLs to crawl") do |u|
    options.urls = File.expand_path(u)
  end

  opts.on("-d", "--depth N", Integer, "Crawl depth",
                                      "  (default: #{options.depth})") do |d|
    options.depth = d
  end

  opts.on("-p", "--processes N", Integer, "Number of (fetch) processes to use",
                                      "  (default: #{options.processes})") do |p|
    options.processes = p
  end

  opts.on("--keep-rejected-urls", "Keep rejected URLs") do 
    options.keep_rejected_urls = true 
  end

  opts.on("--compile", "Compiles all relevant files") do
    options.compile = true 
  end

  opts.on("--cat-domain DIR", "Returns on stdout all data of that domain") do |dom|
    options.cat_domain = dom 
  end

  opts.on("--daemonize", FalseClass, "Run in background, daemonized") do
    options.daemonize = true 
  end

  opts.on_tail("-h", "--help", "Show this message") do
    puts opts
    exit
  end
end

args = ARGV.dup
opts.parse!(args)
if !args.empty?
  puts opts
  exit
end

if options.cat_domain
  dir = File.join(options.root_dir, 'files', Cryl.domain_to_path(options.cat_domain))
  system("find #{ dir } -name '*.data' | xargs -I % cat %")
  exit
end

if options.urls.nil?
  puts "No URLs given (--urls)"
  puts
  puts opts
  exit
end

if options.job_name.nil?
  puts "No Job name given (--job-name)"
  puts
  puts opts
  exit
end

def log(msg)
  if $log
    $log.puts("#{Time.now}: #{msg}")
    $log.flush
  end
end

if options.daemonize
  require 'vendor/daemonize'
  class Daemons; include Daemonize end

  FileUtils.mkdir_p(File.join(options.root_dir, "jobs", options.job_name))
  $log = File.open(File.join(options.root_dir, "jobs", options.job_name, "log"), 'w+')
else
  $log = STDOUT
end

log("command-line: #{ ARGV.join(" ") }")

if options.compile
  log("starting to compile...")
  system("cd #{File.dirname(__FILE__)} && make all")
end


cryl = Cryl.new(options.root_dir, options.job_name, options.processes, options.keep_rejected_urls)

log("going to background")
if options.daemonize
  Daemons.new.daemonize(0, true)
  # reopen log file
  $log = File.open(File.join(options.root_dir, "jobs", options.job_name, "log"), 'a+')
end

log("daemonized")
log("PID: #{Process.pid}")

log("starting the crawl")
begin
  cryl.crawl(options.urls, options.depth, 0)
rescue Exception => e
  log("exception occured")
  log(e.inspect)
  log(e.backtrace.inspect)
  exit -1
end
log("finished")
